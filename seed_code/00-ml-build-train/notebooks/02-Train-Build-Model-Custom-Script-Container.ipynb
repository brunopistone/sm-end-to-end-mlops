{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training ML Model with Custom Script Container\n",
    "\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "In this exercise you will do:\n",
    " - Run a Preprocessing Job using Amazon SageMaker Processing Job\n",
    " - Run a Tensorflow Training Job using Amazon SageMaker Training Job\n",
    " - Register a new version of the trained model in the Amazon SageMaker Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1/2 - Setup\n",
    "Here we'll import some libraries and define some variables. You can also take a look on the scripts that were previously created for preparing the data and training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator, Framework\n",
    "from sagemaker import get_execution_role, image_uris\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import time\n",
    "import traceback\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Global configurations\n",
    "\n",
    "Configuration variables used for Processing, Training, and registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "role_name = \"mlops-sagemaker-execution-role\"\n",
    "role = \"arn:aws:iam::{}:role/{}\".format(boto3.client('sts').get_caller_identity().get('Account'), role_name)\n",
    "\n",
    "kms_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "kms_alias = \"ml-kms\"\n",
    "\n",
    "bucket_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=runtime_client,\n",
    "    default_bucket=bucket_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kms_key = \"arn:aws:kms:{}:{}:alias/{}\".format(region, kms_account_id, kms_alias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2/2: Run the end to end ML workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 1/3: Create the Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processing_entrypoint = \"./../algorithms/processing/src/processing.py\"\n",
    "processing_framework_version = \"0.23-1\"\n",
    "processing_instance_count = 1\n",
    "processing_instance_type = \"ml.t3.large\"\n",
    "processing_input_files_path = \"data/input\"\n",
    "processing_output_files_path = \"data/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Get the dataset and upload it to an S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download the \n",
    "# clean the buckets first\n",
    "s3_client.delete_object(Bucket=bucket_name, Key=processing_input_files_path)\n",
    "\n",
    "input_data = sagemaker_session.upload_data('./../data/TheSocialDilemma.csv', key_prefix=processing_input_files_path)\n",
    "\n",
    "LOGGER.info(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create Processor\n",
    "\n",
    "Lets create a SKLearn Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pygmentize ./../algorithms/processing/src/processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processor = SKLearnProcessor(\n",
    "    framework_version=processing_framework_version,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    "    output_kms_key=kms_key,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processor.run(\n",
    "    code=processing_entrypoint,\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output\", \n",
    "            source=\"/opt/ml/processing/output\", \n",
    "            destination=\"s3://{}/{}\".format(bucket_name, processing_output_files_path))\n",
    "    ],\n",
    "    arguments = [\n",
    "        \"--input-data\",\n",
    "        \"s3://{}/{}\".format(bucket_name, processing_input_files_path)\n",
    "    ],\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 2/3: Create the Trining Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Compress source code for installing additional python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pygmentize ./../algorithms/training/src/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ./../algorithms/buildspec.sh training $bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create Estimator\n",
    "\n",
    "Lets start a training job using a Framework Estimator. Framework estimator allows you to use a custom container created for installing additional dependencies. For providing custom scripts to our container, we have to define the arguments `sagemaker_program` and `sagemaker_submit_directory` as hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Dockerfile\n",
    "\n",
    "The Dockerfile defined is creating starting from the public tensorflow 2.4.1 image, and by the usage of [sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit) we are making our container compatibile for Amazon SageMaker"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pygmentize ./../algorithms/training/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_artifact_path = \"artifact/training\"\n",
    "training_artifact_name = \"sourcedir.tar.gz\"\n",
    "training_output_files_path = \"models\"\n",
    "training_framework_version = \"2.4\"\n",
    "training_python_version = \"py37\"\n",
    "training_instance_count = 1\n",
    "training_instance_type = \"ml.p2.xlarge\"\n",
    "training_hyperparameters = {\n",
    "    \"sagemaker_program\": \"train.py\",\n",
    "    \"sagemaker_submit_directory\": \"s3://{}/{}/{}\".format(bucket_name,\n",
    "                                      training_artifact_path,\n",
    "                                      training_artifact_name\n",
    "                                      ),\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rage\": 3e-5,\n",
    "    \"batch_size\": 100,\n",
    "    \"input_file\": \"processed_data.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "estimator = Estimator(\n",
    "    image_uri=\"691148928602.dkr.ecr.eu-west-1.amazonaws.com/bert-training-toolkit:latest\",\n",
    "    output_path=\"s3://{}/{}\".format(bucket_name,\n",
    "                                    training_output_files_path),\n",
    "    hyperparameters=json_encode_hyperparameters(training_hyperparameters),\n",
    "    enable_sagemaker_metrics=True,\n",
    "    metric_definitions=[\n",
    "        {\n",
    "            'Name': 'Test accuracy',\n",
    "            'Regex': 'Test accuracy:.* ([0-9\\\\.]+)'\n",
    "        }\n",
    "    ],\n",
    "    role=role,\n",
    "    instance_count=training_instance_count,\n",
    "    instance_type=\"local\",\n",
    "    output_kms_key=kms_key,\n",
    "    disable_profiler=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": \"s3://{}/{}\".format(\n",
    "            bucket_name,\n",
    "            processing_output_files_path\n",
    "        ),\n",
    "    },\n",
    "    logs=\"Rules\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 3/3: Register Model in the Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "model_package_group_name = \"ml-end-to-end-group\"\n",
    "model_approval_status = \"PendingManualApproval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "estimator.register(\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[inference_instance_type],\n",
    "    transform_instances=[inference_instance_type]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have just seen how to process, train, and version ML models by using Amazon SageMaker Jobs. Now we are ready to execute our end to end workflow using an Amazon SageMaker Pipeline\n",
    "\n",
    " > [SageMaker-Pipeline](./03-SageMaker-Pipeline-Training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "9c7610616261b649411a39c2b20b34340be0a56af53036b6fd0a93e0d2297f70"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}