{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data exploration/visualization\n",
    "\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "The challenge we're trying to address here is to identify the sentiment from Tweets. \n",
    "The dataset used is a public dataset taken from [Kaggle](https://www.kaggle.com/code/sagniksanyal/tweet-s-text-classicifaction/data)\n",
    "Each data is like:\n",
    " - Username\n",
    " - User location\n",
    " - User description\n",
    " - User creation date\n",
    " - User followers\n",
    " - User friends\n",
    " - User favourites\n",
    " - User is verified\n",
    " - Date of the tweet\n",
    " - Text of the tweet\n",
    " - Sentiment associated to the t\n",
    "\n",
    "Let's start preparing our dataset, then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Let's take a look on the data\n",
    "Loading the dataset using Pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import emoji\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./../data/{}\".format(file_name),\n",
    "    sep=\",\",\n",
    "    quotechar='\"',\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    escapechar='\\\\',\n",
    "    encoding='utf-8',\n",
    "    error_bad_lines=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ploting data, just to have an idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ds = df['source'].value_counts().reset_index()\n",
    "ds.columns = ['source', 'count']\n",
    "ds = ds.sort_values(['count'],ascending=False)\n",
    "\n",
    "fig = sns.barplot(\n",
    "    x=ds.head(10)[\"count\"], \n",
    "    y=ds.head(10)[\"source\"], \n",
    "    orientation='horizontal', \n",
    ").set_title('Top 10 user sources by number of tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preparation\n",
    "Now lets clean the text content from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.lstrip()\n",
    "    text = text.rstrip()\n",
    "\n",
    "    text = re.sub(\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U0001F1F2\"\n",
    "                               u\"\\U0001F1F4\"\n",
    "                               u\"\\U0001F620\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    text = emoji.replace_emoji(text, \"\")\n",
    "\n",
    "    text = text.replace(\"u'\", \"'\")\n",
    "\n",
    "    text = text.encode(\"ascii\", \"ignore\")\n",
    "    text = text.decode()\n",
    "\n",
    "    word_list = text.split(' ')\n",
    "\n",
    "    for word in word_list:\n",
    "        if isinstance(word, bytes):\n",
    "            word = word.decode(\"utf-8\")\n",
    "\n",
    "    text = \" \".join(word_list)\n",
    "\n",
    "    if not any(c.isalpha() for c in text):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date):\n",
    "    date = time.mktime(datetime.datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\").timetuple())\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df[[\"user_name\", \"date\", \"text\", \"Sentiment\"]]\n",
    "\n",
    "LOGGER.info(\"Original count: {}\".format(len(df.index)))\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df[\"user_name\"] = df[\"user_name\"].apply(lambda x: clean_text(x))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "df['user_name'] = df['user_name'].map(lambda x: x.strip())\n",
    "df['user_name'] = df['user_name'].replace('', np.nan)\n",
    "df['user_name'] = df['user_name'].replace(' ', np.nan)\n",
    "\n",
    "df['date'] = df['date'].map(lambda x: x.strip())\n",
    "df['date'] = df['date'].replace('', np.nan)\n",
    "df['date'] = df['date'].replace(' ', np.nan)\n",
    "df[\"date\"] = df[\"date\"].apply(lambda x: convert_date(x))\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: x.strip())\n",
    "df['text'] = df['text'].replace('', np.nan)\n",
    "df['text'] = df['text'].replace(' ', np.nan)\n",
    "\n",
    "df['Sentiment'] = df['Sentiment'].map(lambda x: x.strip())\n",
    "df['Sentiment'] = df['Sentiment'].replace('', np.nan)\n",
    "df['Sentiment'] = df['Sentiment'].replace(' ', np.nan)\n",
    "\n",
    "df[\"Sentiment\"] = df[\"Sentiment\"].map({\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2})\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "LOGGER.info(\"Current count: {}\".format(len(df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ploting cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have just cleaned and explored our dataset. Now lets move on and see how to process data using Amazon SageMaker Processing Jobs\n",
    "\n",
    " > [Prepare-Data-ML](./01-Prepare-Data-ML.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have just cleaned and explored our dataset. Now lets move on and see how to process data using Amazon SageMaker Processing Jobs\n",
    "\n",
    " > [Prepare-Data-ML](./01-Prepare-Data-ML.ipynb)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}