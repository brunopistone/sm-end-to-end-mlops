{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Explainability - Amazon SageMaker Clarify\n",
    "\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "In this exercise you will do:\n",
    " - Create an Amazon SageMaker Clarify job for evaluating feature importance for your model\n",
    " - Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1/4 - Setup\n",
    "Here we'll import some libraries and define some variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "from io import StringIO\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sagemaker.session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role_name = \"mlops-sagemaker-execution-role\"\n",
    "role = \"arn:aws:iam::{}:role/{}\".format(boto3.client('sts').get_caller_identity().get('Account'), role_name)\n",
    "\n",
    "kms_account_id = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2/4 - Create Model Predictor\n",
    "During this step, we are creating a model predictor for a previously created SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bucket_artifacts = \"\"\n",
    "bucket_inference = \"\"\n",
    "\n",
    "clarify_path = \"data/clarify\"\n",
    "\n",
    "explainability_output_path = 's3://{}/data/monitoring/explainability'.format(bucket_artifacts)\n",
    "\n",
    "inference_instance_count = 1\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "kms_alias = \"ml-kms\"\n",
    "\n",
    "model_package_group = \"ml-end-to-end-group\"\n",
    "\n",
    "monitoring_output_path = \"data/monitoring/captured\"\n",
    "\n",
    "processing_output_files_path = \"data/output\"\n",
    "\n",
    "train_data = \"s3://{}/{}/train/train.csv\".format(bucket_artifacts, processing_output_files_path)\n",
    "test_data = \"s3://{}/{}/test/test.csv\".format(bucket_artifacts, processing_output_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kms_key = \"arn:aws:kms:{}:{}:alias/{}\".format(region, kms_account_id, kms_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=runtime_client,\n",
    "    default_bucket=bucket_inference\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Negative - 0\n",
    "* Neutral - 1\n",
    "* Positive - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(\n",
    "    endpoint_name=model_package_group + \"-dev\",\n",
    "    model_name=\"saved_model\",\n",
    "    model_version=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs = [\"ti imploro di guardare questo documentario. molto spaventoso e informativo. uno dei motivi esatti che sto eliminando fb entir\"]\n",
    "\n",
    "result = predictor.predict(inputs)\n",
    "\n",
    "LOGGER.info(\"{}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 3/4 - Explainability\n",
    "Here we are creating Amazon SageMaker Clarify jobs for model explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare data for Amazon SageMaker Clarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "                test_data,\n",
    "                sep=',',\n",
    "                quotechar='\"',\n",
    "                quoting=csv.QUOTE_ALL,\n",
    "                escapechar='\\\\',\n",
    "                encoding='utf-8',\n",
    "                error_bad_lines=False\n",
    "            )\n",
    "        \n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test[\"len\"] = df_test[\"text\"].apply(lambda ele: len(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 300\n",
    "\n",
    "df_test_clarify = pd.DataFrame(\n",
    "    df_test.sample(n=num_examples),\n",
    "    columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "\n",
    "df_test_clarify.to_csv(csv_buffer, header=True, index=False)\n",
    "\n",
    "s3_resource.Object(bucket_inference, \"{}/validation.csv\".format(clarify_path)).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create Amazon SageMaker Clarify Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.clarify import (\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    SageMakerClarifyProcessor,\n",
    "    SHAPConfig,\n",
    "    TextConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To obtain feature importance for parts of an input text, create a TextConfig specifying the granularity of the parts of the text and the language. Clarify then breaks the text down into tokens, sentences, or paragraphs depending on your choice of granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_config = TextConfig(\n",
    "    language=\"english\", \n",
    "    granularity=\"sentence\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap_config = SHAPConfig(\n",
    "    baseline=[[\"<UNK>\"]],\n",
    "    num_samples=1000,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True,\n",
    "    text_config=text_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "explainability_data_config = DataConfig(\n",
    "    s3_data_input_path=\"s3://{}/{}/validation.csv\".format(bucket_inference, clarify_path),\n",
    "    s3_output_path=explainability_output_path,\n",
    "    headers=[\"text\"],\n",
    "    dataset_type=\"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    model_name=predictor._get_model_names()[0],\n",
    "    instance_type=inference_instance_type,  \n",
    "    instance_count=inference_instance_count,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run an Amazon SageMaker Clarify Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clarify_processor = SageMakerClarifyProcessor(\n",
    "    role=role, \n",
    "    instance_count=inference_instance_count, \n",
    "    instance_type=inference_instance_type, \n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 4/4 - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize local explanations\n",
    "\n",
    "We use Captum to visualize the feature importances computed by Clarify. First, lets load the local explanations. Local text explanations can be found in the analysis results folder in a file named out.jsonl in the explanations_shap directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "local_feature_attributions_file = \"out.jsonl\"\n",
    "analysis_results = []\n",
    "analysis_result = sagemaker.s3.S3Downloader.download(\n",
    "    explainability_output_path + \"/explanations_shap/\" + local_feature_attributions_file,\n",
    "    local_path=\"./../data\",\n",
    ")\n",
    "\n",
    "shap_out = []\n",
    "file = sagemaker.s3.S3Downloader.read_file(\n",
    "    explainability_output_path + \"/explanations_shap/\" + local_feature_attributions_file\n",
    ")\n",
    "for line in file.split(\"\\n\"):\n",
    "    if line:\n",
    "        shap_out.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The local explanations file is a JSON Lines file, that contains the explanation of one instance per row. Let's examine the output format of the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(json.dumps(shap_out[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At the highest level of this JSON Line, there are two keys: explanations, join_source_value (Not present here as we have not included a joinsource column in the input dataset). explanations contains a list of attributions for each feature in the dataset. In this case, we have a single element, because the input dataset also had a single feature. It also contains details like feature_name, data_type of the features (indicating whether Clarify inferred the column as numerical, categorical or text). Each token attribution also contains a description field that contains the token itself, and the starting index of the token in original input. This allows you to reconstruct the original sentence from the output as well.\n",
    "In the following block, we create a list of attributions and a list of tokens for use in visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attributions_dataset = [\n",
    "    np.array([attr[\"attribution\"][0] for attr in expl[\"explanations\"][0][\"attributions\"]])\n",
    "    for expl in shap_out\n",
    "]\n",
    "tokens_dataset = [\n",
    "    np.array(\n",
    "        [attr[\"description\"][\"partial_text\"] for attr in expl[\"explanations\"][0][\"attributions\"]]\n",
    "    )\n",
    "    for expl in shap_out\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We obtain predictions as well so that they can be displayed alongside the feature attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for t in df_test_clarify.values:\n",
    "    preds.append(predictor.predict([t]))\n",
    "    print(\".\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from captum.attr import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# This method is a wrapper around the captum that helps produce visualizations for local explanations. It will\n",
    "# visualize the attributions for the tokens with red or green colors for negative and positive attributions.\n",
    "def visualization_record(\n",
    "    attributions,  # list of attributions for the tokens\n",
    "    text,  # list of tokens\n",
    "    pred,  # the prediction value obtained from the endpoint\n",
    "    delta,\n",
    "    true_label,  # the true label from the dataset\n",
    "    normalize=True,  # normalizes the attributions so that the max absolute value is 1. Yields stronger colors.\n",
    "    max_frac_to_show=0.05,  # what fraction of tokens to highlight, set to 1 for all.\n",
    "    match_to_pred=False,  # whether to limit highlights to red for negative predictions and green for positive ones.\n",
    "    # By enabling `match_to_pred` you show what tokens contribute to a high/low prediction not those that oppose it.\n",
    "):\n",
    "    if normalize:\n",
    "        attributions = attributions / max(max(attributions), max(-attributions))\n",
    "    if max_frac_to_show is not None and max_frac_to_show < 1:\n",
    "        num_show = int(max_frac_to_show * attributions.shape[0])\n",
    "        sal = attributions\n",
    "        if pred < 0.5:\n",
    "            sal = -sal\n",
    "        if not match_to_pred:\n",
    "            sal = np.abs(sal)\n",
    "        top_idxs = np.argsort(-sal)[:num_show]\n",
    "        mask = np.zeros_like(attributions)\n",
    "        mask[top_idxs] = 1\n",
    "        attributions = attributions * mask\n",
    "    return visualization.VisualizationDataRecord(\n",
    "        attributions,\n",
    "        pred,\n",
    "        int(pred > 0.5),\n",
    "        true_label,\n",
    "        attributions.sum() > 0,\n",
    "        attributions.sum(),\n",
    "        text,\n",
    "        delta,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You can customize the following display settings\n",
    "normalize = True\n",
    "max_frac_to_show = 1\n",
    "match_to_pred = False\n",
    "labels = df_test[\"Sentiment\"][:num_examples]\n",
    "vis = []\n",
    "for attr, token, pred, label in zip(attributions_dataset, tokens_dataset, preds, labels):\n",
    "    vis.append(\n",
    "        visualization_record(\n",
    "            attr, token, float(pred[0][1]), 0.0, label, normalize, max_frac_to_show, match_to_pred\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we compiled the record we are finally ready to render the visualization.\n",
    "\n",
    "We see a row per review in the selected dataset. For each row we have the prediction, the label, and the highlighted text. Additionally, we show the total sum of attributions (as attribution score) and its label (as attribution label), which indicates whether it is greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = visualization.visualize_text(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -rf ./../data"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "7830b2e0dcc405ab83456d8c26dd7c2db32ddf1a7b2e64ef505b215ebac66515"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}