{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy our ML Model\n",
    "\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "In this exercise you will do:\n",
    " - Run a Preprocessing Job using Amazon SageMaker Processing Job\n",
    " - Run a Tensorflow Training Job using Amazon SageMaker Training Job\n",
    " - Register a new version of the trained model in the Amazon SageMaker Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1/4 - Setup\n",
    "Here we'll import some libraries and define some variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model_monitor import DataCaptureConfig, EndpointInput\n",
    "import sagemaker.session\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "sagemaker_client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2/4 - Create Model Predictor\n",
    "During this step, we are creating a model predictor for a previously created SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "role_name = \"mlops-sagemaker-execution-role\"\n",
    "role = \"arn:aws:iam::{}:role/{}\".format(boto3.client('sts').get_caller_identity().get('Account'), role_name)\n",
    "\n",
    "kms_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "bucket_inference = \"\"\n",
    "\n",
    "kms_alias = \"ml-kms\"\n",
    "\n",
    "model_package_group = \"ml-end-to-end-group\"\n",
    "\n",
    "monitoring_output_path = \"data/monitoring/captured\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kms_key = \"arn:aws:kms:{}:{}:alias/{}\".format(region, kms_account_id, kms_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=runtime_client,\n",
    "    default_bucket=bucket_inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(\n",
    "    endpoint_name=model_package_group + \"-dev\",\n",
    "    model_name=\"saved_model\",\n",
    "    model_version=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Sei disgustoso\"]\n",
    "\n",
    "result = predictor.predict(inputs)\n",
    "\n",
    "LOGGER.info(\"{}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3/4 - Monitoring\n",
    "Here we are creating monitoring jobs for extracting metrics from our SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Create a Baseline for the monitoring job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our train dataset, let's select the relevant attributes and generate a dataset for baselining. Then we use Amazon SageMaker Model Monitor to suggest a set of baseline constraints and descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker.model_monitor import CronExpressionGenerator, ModelQualityMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_artifacts = \"\"\n",
    "bucket_inference = \"\"\n",
    "\n",
    "monitoring_input_files_path = \"data/monitoring/input\"\n",
    "processing_output_files_path = \"data/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"s3://{}/{}/train/train.csv\".format(bucket_artifacts, processing_output_files_path)\n",
    "\n",
    "columns = [\"text\", \"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_output_path = \"s3://{}/data/monitoring/output\".format(bucket_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute predictions using the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/validation_with_predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 200  # Need at least 200 samples to compute standard deviations\n",
    "i = 0\n",
    "\n",
    "header = True\n",
    "\n",
    "try:\n",
    "    with open(\"/tmp/validation_with_predictions.csv\", \"w\") as baseline_file:\n",
    "        baseline_file.write(\"probability,prediction,label\\n\")\n",
    "        \n",
    "        df = pd.read_csv(train_data, usecols=columns)\n",
    "        df = df.dropna()\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if header:\n",
    "                header = False\n",
    "            else:\n",
    "                text = row[\"text\"]\n",
    "                Sentiment = row[\"Sentiment\"]\n",
    "\n",
    "                inputs = [text]\n",
    "\n",
    "                results = predictor.predict(inputs)\n",
    "\n",
    "                probability = results[0][1]\n",
    "                prediction = results[0][0]\n",
    "                \n",
    "                baseline_file.write(f\"{probability},{prediction},{Sentiment}\\n\")\n",
    "                i += 1\n",
    "                if i > limit:\n",
    "                    break\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    LOGGER.info(\"Done!\")\n",
    "except Exception as e:\n",
    "    print(text)\n",
    "    print(Sentiment)\n",
    "    stacktrace = traceback.format_exc()\n",
    "    LOGGER.error(\"{}\".format(stacktrace))\n",
    "\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head /tmp/validation_with_predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_input_files_path = \"data/monitoring/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.delete_object(Bucket=bucket_inference, Key=monitoring_input_files_path)\n",
    "\n",
    "baseline_dataset_uri = sagemaker_session.upload_data('/tmp/validation_with_predictions.csv', key_prefix=monitoring_input_files_path)\n",
    "\n",
    "print(baseline_dataset_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that running the baselining job will require 8-10 minutes. In the meantime, you can take a look at the Deequ library, used to execute these analyses with the default Model Monitor container: https://github.com/awslabs/deequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_dataset_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_output_path,\n",
    "    problem_type=\"MulticlassClassification\",\n",
    "    inference_attribute=\"prediction\",\n",
    "    probability_attribute=\"probability\",\n",
    "    ground_truth_attribute=\"label\",\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the results of the baselining job\n",
    "You could see the baseline constraints and statistics files are uploaded to the S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View the metrics generated\n",
    "You could see that the baseline statistics and constraints files are already uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = baseline_job.baseline_statistics().body_dict[\"multiclass_classification_metrics\"]\n",
    "pd.json_normalize(metrics).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View the constraints generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(baseline_job.suggested_constraints().body_dict[\"multiclass_classification_constraints\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prediction data for Model Quality Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"s3://{}/{}/test/test.csv\".format(bucket_artifacts, processing_output_files_path)\n",
    "\n",
    "columns = [\"text\", \"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 200\n",
    "i = 0\n",
    "\n",
    "header = True\n",
    "\n",
    "try:\n",
    "        \n",
    "    df = pd.read_csv(train_data, usecols=columns)\n",
    "    df = df.dropna()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if header:\n",
    "            header = False\n",
    "        else:\n",
    "            text = row[\"text\"]\n",
    "            Sentiment = row[\"Sentiment\"]\n",
    "\n",
    "            inputs = [text]\n",
    "\n",
    "            predictor.predict(inputs)\n",
    "            i += 1\n",
    "            if i > limit:\n",
    "                break\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    LOGGER.info(\"Done!\")\n",
    "except Exception as e:\n",
    "    print(text)\n",
    "    print(Sentiment)\n",
    "    stacktrace = traceback.format_exc()\n",
    "    LOGGER.error(\"{}\".format(stacktrace))\n",
    "\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View captured data\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_inference = \"\"\n",
    "\n",
    "endpoint_name = model_package_group + \"-dev\"\n",
    "\n",
    "monitoring_output_path = \"data/monitoring/captured\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving eventIds in a list for creating a grount truth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for captures to show up\", end=\"\")\n",
    "\n",
    "event_ids = []\n",
    "\n",
    "for _ in range(120):\n",
    "    capture_files = sorted(S3Downloader.list(\"s3://{}/{}/{}\".format(bucket_inference, monitoring_output_path, endpoint_name)))\n",
    "    if capture_files:\n",
    "        capture_file = S3Downloader.read_file(capture_files[-1]).split(\"\\n\")\n",
    "        capture_record = json.loads(capture_file[0])\n",
    "        if \"inferenceId\" in capture_record[\"eventMetadata\"] or \"eventId\" in capture_record[\"eventMetadata\"]:\n",
    "            ids = []\n",
    "            for record in capture_file:\n",
    "                try:\n",
    "                    record = json.loads(record)\n",
    "\n",
    "                    event_ids.append(record[\"eventMetadata\"][\"eventId\"])\n",
    "                except:\n",
    "                    pass\n",
    "            break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(1)\n",
    "\n",
    "    \n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files[-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file so that you can observe a little better.\n",
    "\n",
    "Again, notice the `inferenceId` or `eventId` attribute that is set as part of the invoke_endpoint call.  If this is present, it will be used to join with ground truth data (otherwise `eventId` will be used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(capture_file[-3:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(capture_record, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic ground truth\n",
    "\n",
    "Next, start generating ground truth data. The model quality job will fail if there's no ground truth data to merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_upload_path = \"data/monitoring/ground_truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def ground_truth_with_id(inference_id):\n",
    "    random.seed(inference_id)  # to get consistent results\n",
    "    rand = random.random()\n",
    "    return {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": \"{},1.0\".format(random.sample(range(0, 3), 1)[0]),\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\n",
    "            \"eventId\": str(inference_id),\n",
    "        },\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "\n",
    "def upload_ground_truth(records, upload_time):\n",
    "    fake_records = [json.dumps(r) for r in records]\n",
    "    data_to_upload = \"\\n\".join(fake_records)\n",
    "    target_s3_uri = f\"s3://{bucket_inference}/{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "    print(f\"Uploading {len(fake_records)} records to\", target_s3_uri)\n",
    "    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_records = []\n",
    "\n",
    "for el in range(len(event_ids) - 1):    \n",
    "    event_id = event_ids.pop()\n",
    "    \n",
    "    result = ground_truth_with_id(event_id)\n",
    "    \n",
    "    fake_records.append(result)\n",
    "\n",
    "upload_ground_truth(fake_records, datetime.utcnow())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Monitoring Scheduler\n",
    "\n",
    "Here we are creating our monitoring scheduler. It will execute monitoring jobs with hourly schedule execution. When we create the schedule, we can also specify two scripts that will preprocess the records before the analysis takes place and execute post-processing at the end. For this example, we are not going to use a record preprocessor, and we are just specifying a post-processor that outputs some text for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_path = \"s3://{}/data/monitoring/reports\".format(bucket_inference)\n",
    "\n",
    "LOGGER.info(reports_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an enpointInput\n",
    "endpointInput = EndpointInput(\n",
    "    endpoint_name=predictor.endpoint_name,\n",
    "    inference_attribute=\"0\",\n",
    "    probability_attribute=\"1\",\n",
    "    probability_threshold_attribute=0.5,\n",
    "    destination=\"/opt/ml/processing/input_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "\n",
    "mon_schedule_name = \"\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=endpointInput,\n",
    "    #record_preprocessor_script=preprocessor_path,\n",
    "    #post_analytics_processor_script=postprocessor_path,\n",
    "    problem_type=\"MulticlassClassification\",\n",
    "    output_s3_uri=\"s3://{}/{}\".format(bucket_inference, reports_path),\n",
    "    ground_truth_input=\"s3://{}/{}\".format(bucket_inference, ground_truth_upload_path),\n",
    "    constraints=monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = monitor.describe_schedule()\n",
    "desc_schedule_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = monitor.list_executions()[-1]\n",
    "report_uri = latest_execution.describe()[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n",
    "    \"S3Uri\"\n",
    "]\n",
    "print(\"Report Uri:\", report_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "violations = latest_execution.constraint_violations().body_dict[\"violations\"]\n",
    "violations_df = pd.json_normalize(violations)\n",
    "violations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Scheduler\n",
    "\n",
    "Once the schedule is created, it will kick of jobs at specified intervals. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait till you cross the hour boundary (in UTC) to see executions kick off. Since we don't want to wait for the hour in this example we can delete the schedule and use the code in next steps to simulate what will happen when a schedule is triggered, by running an Amazon SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual monitoring execution\n",
    "\n",
    "In oder to trigger the execution manually, we first get all paths to data capture, baseline statistics, baseline constraints, etc. Then, we use a utility fuction, defined in monitoringjob_utils.py, to run the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('./../scripts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoringjob_utils import run_model_monitor_job_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_artifacts = \"\"\n",
    "bucket_inference = \"\"\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "current_endpoint_capture_prefix = \"data/monitoring/captured/{}\".format(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_code_prefix = \"artifact/monitoring\"\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket_artifacts).Object(monitoring_code_prefix + \"/preprocess.py\").upload_file(\"./../algorithms/monitoring/src/preprocess.py\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket_artifacts).Object(monitoring_code_prefix + \"/postprocess.py\").upload_file(\"./../algorithms/monitoring/src/postprocess.py\")\n",
    "\n",
    "preprocessor_path = \"s3://{}/{}/preprocess.py\".format(bucket_artifacts, monitoring_code_prefix)\n",
    "postprocessor_path = \"s3://{}/{}/postprocess.py\".format(bucket_artifacts, monitoring_code_prefix)\n",
    "\n",
    "LOGGER.info(preprocessor_path)\n",
    "LOGGER.info(postprocessor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "role_name = \"mlops-sagemaker-execution-role\"\n",
    "role = \"arn:aws:iam::{}:role/{}\".format(boto3.client('sts').get_caller_identity().get('Account'), role_name)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket_inference, Prefix=current_endpoint_capture_prefix)\n",
    "\n",
    "capture_files = [\"s3://{0}/{1}\".format(bucket_inference, capture_file.get(\"Key\")) for capture_file in result.get(\"Contents\")]\n",
    "data_capture_path = capture_files[len(capture_files) - 1][: capture_files[len(capture_files) - 1].rfind('/')]\n",
    "\n",
    "ground_truth_upload_path = \"data/monitoring/ground_truth\"\n",
    "result = s3_client.list_objects(Bucket=bucket_inference, Prefix=ground_truth_upload_path)\n",
    "\n",
    "ground_truth_files = [\"s3://{0}/{1}\".format(bucket_inference, capture_file.get(\"Key\")) for capture_file in result.get(\"Contents\")]\n",
    "ground_truth_path = ground_truth_files[len(ground_truth_files) - 1][: ground_truth_files[len(ground_truth_files) - 1].rfind('/')]\n",
    "\n",
    "LOGGER.info(\"Capture Files: \")\n",
    "LOGGER.info(\"\\n \".join(capture_files))\n",
    "LOGGER.info(\"Ground Truth path: {}\".format(ground_truth_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_monitor_job_processor(\n",
    "    region, \n",
    "    \"ml.m5.xlarge\", \n",
    "    role, \n",
    "    endpoint_name,\n",
    "    data_capture_path, \n",
    "    ground_truth_path,\n",
    "    reports_path,\n",
    "    #preprocessor_path=preprocessor_path,\n",
    "    #postprocessor_path=postprocessor_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Here we are analyzing the report created by our Monitoring Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_inference = \"\"\n",
    "\n",
    "reports_path = \"data/monitoring/reports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket_inference, Prefix=reports_path)\n",
    "\n",
    "try:\n",
    "    monitoring_reports = ['s3://{0}/{1}'.format(bucket_inference, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "    print(\"Monitoring Reports Files: \")\n",
    "    print(\"\\n \".join(monitoring_reports))\n",
    "except:\n",
    "    print('No monitoring reports found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {monitoring_reports[0]} ./../data/monitoring/\n",
    "!aws s3 cp {monitoring_reports[1]} ./../data/monitoring/\n",
    "!aws s3 cp {monitoring_reports[2]} ./../data/monitoring/\n",
    "!aws s3 cp {monitoring_reports[3]} ./../data/monitoring/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "file = open('./../data/monitoring/constraint_violations.json', 'r')\n",
    "data = file.read()\n",
    "\n",
    "violations_df = pd.json_normalize(json.loads(data)['violations'])\n",
    "violations_df"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "7830b2e0dcc405ab83456d8c26dd7c2db32ddf1a7b2e64ef505b215ebac66515"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
