{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deployment Pipeline\n",
    "\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "In this exercise you will do:\n",
    " - Run the end to end workflow described in the previous exercise for deploying an Amazon SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1/3 - Setup\n",
    "Here we'll import some libraries and define some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('./../mlpipelines'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from deployment.pipeline import get_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2/2 - Run the Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Explore Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "\u001B[33mThis sample is non-production-ready template\u001B[39;49;00m\n",
      "\u001B[33m© 2021 Amazon Web Services, Inc. or its affiliates. All Rights Reserved.\u001B[39;49;00m\n",
      "\u001B[33mThis AWS Content is provided subject to the terms of the AWS Customer Agreement available at\u001B[39;49;00m\n",
      "\u001B[33mhttp://aws.amazon.com/agreement or other written agreement between Customer and either\u001B[39;49;00m\n",
      "\u001B[33mAmazon Web Services, Inc. or Amazon Web Services EMEA SARL or both.\u001B[39;49;00m\n",
      "\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mboto3\u001B[39;49;00m\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mbotocore\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mexceptions\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m ClientError\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mdatetime\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m datetime\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mlogging\u001B[39;49;00m\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36msagemaker\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m get_execution_role\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36msagemaker\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mmodel_monitor\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m DataCaptureConfig\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36msagemaker\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36msession\u001B[39;49;00m\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36msagemaker\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mtensorflow\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m TensorFlowModel\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtraceback\u001B[39;49;00m\n",
      "\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "LOGGER = logging.getLogger(\u001B[31m__name__\u001B[39;49;00m)\n",
      "\n",
      "sagemaker_client = boto3.client(\u001B[33m\"\u001B[39;49;00m\u001B[33msagemaker\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\n",
      "\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mget_kms_key\u001B[39;49;00m(\n",
      "        region,\n",
      "        account_id,\n",
      "        kms_alias):\n",
      "    \u001B[34mtry\u001B[39;49;00m:\n",
      "        kms_key = \u001B[33m\"\u001B[39;49;00m\u001B[33marn:aws:kms:\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m:\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m:alias/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(region, account_id, kms_alias)\n",
      "\n",
      "        \u001B[34mreturn\u001B[39;49;00m kms_key\n",
      "    \u001B[34mexcept\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m e:\n",
      "        stacktrace = traceback.format_exc()\n",
      "        LOGGER.error(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(stacktrace))\n",
      "\n",
      "        \u001B[34mraise\u001B[39;49;00m e\n",
      "\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mdescribe_model_package\u001B[39;49;00m(model_package_arn):\n",
      "    \u001B[34mtry\u001B[39;49;00m:\n",
      "        model_package = sagemaker_client.describe_model_package(\n",
      "            ModelPackageName=model_package_arn\n",
      "        )\n",
      "\n",
      "        LOGGER.info(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(model_package))\n",
      "\n",
      "        \u001B[34mif\u001B[39;49;00m \u001B[36mlen\u001B[39;49;00m(model_package) == \u001B[34m0\u001B[39;49;00m:\n",
      "            error_message = (\u001B[33m\"\u001B[39;49;00m\u001B[33mNo ModelPackage found for: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(model_package_arn))\n",
      "            LOGGER.error(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(error_message))\n",
      "\n",
      "            \u001B[34mraise\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m(error_message)\n",
      "\n",
      "        \u001B[34mreturn\u001B[39;49;00m model_package\n",
      "    \u001B[34mexcept\u001B[39;49;00m ClientError \u001B[34mas\u001B[39;49;00m e:\n",
      "        stacktrace = traceback.format_exc()\n",
      "        error_message = e.response[\u001B[33m\"\u001B[39;49;00m\u001B[33mError\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m][\u001B[33m\"\u001B[39;49;00m\u001B[33mMessage\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]\n",
      "        LOGGER.error(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(stacktrace))\n",
      "\n",
      "        \u001B[34mraise\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m(error_message)\n",
      "\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mget_approved_package\u001B[39;49;00m(model_package_group):\n",
      "    \u001B[33m\"\"\"Gets the latest approved model package for a model package group.\u001B[39;49;00m\n",
      "\u001B[33m\u001B[39;49;00m\n",
      "\u001B[33m    Args:\u001B[39;49;00m\n",
      "\u001B[33m        model_package_group: The model package group name.\u001B[39;49;00m\n",
      "\u001B[33m\u001B[39;49;00m\n",
      "\u001B[33m    Returns:\u001B[39;49;00m\n",
      "\u001B[33m        The SageMaker Model Package ARN.\u001B[39;49;00m\n",
      "\u001B[33m    \"\"\"\u001B[39;49;00m\n",
      "    \u001B[34mtry\u001B[39;49;00m:\n",
      "        \u001B[37m# Get the latest approved model package\u001B[39;49;00m\n",
      "        response = sagemaker_client.list_model_packages(\n",
      "            ModelPackageGroupName=model_package_group,\n",
      "            ModelApprovalStatus=\u001B[33m\"\u001B[39;49;00m\u001B[33mApproved\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\n",
      "            SortBy=\u001B[33m\"\u001B[39;49;00m\u001B[33mCreationTime\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\n",
      "            SortOrder=\u001B[33m\"\u001B[39;49;00m\u001B[33mDescending\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\n",
      "            MaxResults=\u001B[34m1\u001B[39;49;00m,\n",
      "        )\n",
      "        approved_packages = response[\u001B[33m\"\u001B[39;49;00m\u001B[33mModelPackageSummaryList\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]\n",
      "\n",
      "        \u001B[37m# Return error if no packages found\u001B[39;49;00m\n",
      "        \u001B[34mif\u001B[39;49;00m \u001B[36mlen\u001B[39;49;00m(approved_packages) == \u001B[34m0\u001B[39;49;00m:\n",
      "            error_message = (\u001B[33m\"\u001B[39;49;00m\u001B[33mNo approved ModelPackage found for ModelPackageGroup: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(model_package_group))\n",
      "            LOGGER.error(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(error_message))\n",
      "\n",
      "            \u001B[34mraise\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m(error_message)\n",
      "\n",
      "        model_package = approved_packages[\u001B[34m0\u001B[39;49;00m]\n",
      "        LOGGER.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mIdentified the latest approved model package: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(model_package))\n",
      "\n",
      "        \u001B[34mreturn\u001B[39;49;00m model_package\n",
      "    \u001B[34mexcept\u001B[39;49;00m ClientError \u001B[34mas\u001B[39;49;00m e:\n",
      "        stacktrace = traceback.format_exc()\n",
      "        error_message = e.response[\u001B[33m\"\u001B[39;49;00m\u001B[33mError\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m][\u001B[33m\"\u001B[39;49;00m\u001B[33mMessage\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]\n",
      "        LOGGER.error(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(stacktrace))\n",
      "\n",
      "        \u001B[34mraise\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m(error_message)\n",
      "\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mget_session\u001B[39;49;00m(\n",
      "        region,\n",
      "        default_bucket):\n",
      "    \u001B[34mtry\u001B[39;49;00m:\n",
      "        boto_session = boto3.Session(region_name=region)\n",
      "\n",
      "        sagemaker_client = boto_session.client(\u001B[33m\"\u001B[39;49;00m\u001B[33msagemaker\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\n",
      "        runtime_client = boto_session.client(\u001B[33m\"\u001B[39;49;00m\u001B[33msagemaker-runtime\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\n",
      "\n",
      "        \u001B[34mreturn\u001B[39;49;00m sagemaker.session.Session(\n",
      "            boto_session=boto_session,\n",
      "            sagemaker_client=sagemaker_client,\n",
      "            sagemaker_runtime_client=runtime_client,\n",
      "            default_bucket=default_bucket\n",
      "        )\n",
      "    \u001B[34mexcept\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m e:\n",
      "        stacktrace = traceback.format_exc()\n",
      "        LOGGER.error(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(stacktrace))\n",
      "\n",
      "        \u001B[34mraise\u001B[39;49;00m e\n",
      "\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mget_deployed_model\u001B[39;49;00m():\n",
      "    \u001B[34mtry\u001B[39;49;00m:\n",
      "        response = sagemaker_client.list_models(\n",
      "            SortBy=\u001B[33m\"\u001B[39;49;00m\u001B[33mCreationTime\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\n",
      "            SortOrder=\u001B[33m\"\u001B[39;49;00m\u001B[33mDescending\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\n",
      "            MaxResults=\u001B[34m1\u001B[39;49;00m\n",
      "        )\n",
      "\n",
      "        model_name = \u001B[34mNone\u001B[39;49;00m\n",
      "\n",
      "        \u001B[34mif\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mModels\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[35min\u001B[39;49;00m response \u001B[35mand\u001B[39;49;00m \u001B[36mlen\u001B[39;49;00m(response[\u001B[33m\"\u001B[39;49;00m\u001B[33mModels\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]) > \u001B[34m0\u001B[39;49;00m:\n",
      "            model_name = response[\u001B[33m\"\u001B[39;49;00m\u001B[33mModels\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m][\u001B[34m0\u001B[39;49;00m][\u001B[33m\"\u001B[39;49;00m\u001B[33mModelName\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]\n",
      "\n",
      "        \u001B[34mreturn\u001B[39;49;00m model_name\n",
      "    \u001B[34mexcept\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m e:\n",
      "        stacktrace = traceback.format_exc()\n",
      "        LOGGER.error(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(stacktrace))\n",
      "\n",
      "        \u001B[34mraise\u001B[39;49;00m e\n",
      "\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mdeploy_model\u001B[39;49;00m(\n",
      "        bucket_inference,\n",
      "        model,\n",
      "        model_package_group_name,\n",
      "        env,\n",
      "        inference_instance_count,\n",
      "        inference_instance_type,\n",
      "        kms_key,\n",
      "        monitoring_output_path):\n",
      "    \u001B[34mtry\u001B[39;49;00m:\n",
      "        LOGGER.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mDeploying endpoint \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(model_package_group_name + \u001B[33m\"\u001B[39;49;00m\u001B[33m-\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m + env))\n",
      "\n",
      "        model.deploy(\n",
      "            endpoint_name=model_package_group_name + \u001B[33m\"\u001B[39;49;00m\u001B[33m-\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m + env,\n",
      "            initial_instance_count=inference_instance_count,\n",
      "            instance_type=inference_instance_type,\n",
      "            update_endpoint=\u001B[34mTrue\u001B[39;49;00m,\n",
      "            data_capture_config=DataCaptureConfig(\n",
      "                enable_capture=\u001B[34mTrue\u001B[39;49;00m,\n",
      "                sampling_percentage=\u001B[34m100\u001B[39;49;00m,\n",
      "                json_content_types=[\u001B[33m\"\u001B[39;49;00m\u001B[33mapplication/jsonlines\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m],\n",
      "                destination_s3_uri=\u001B[33m\"\u001B[39;49;00m\u001B[33ms3://\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(bucket_inference, monitoring_output_path))\n",
      "        )\n",
      "    \u001B[34mexcept\u001B[39;49;00m ClientError \u001B[34mas\u001B[39;49;00m e:\n",
      "        stacktrace = traceback.format_exc()\n",
      "        LOGGER.info(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(stacktrace))\n",
      "\n",
      "        model_name = get_deployed_model()\n",
      "\n",
      "        update_model(\n",
      "            bucket_inference,\n",
      "            model_name,\n",
      "            model_package_group_name,\n",
      "            env,\n",
      "            inference_instance_count,\n",
      "            inference_instance_type,\n",
      "            kms_key,\n",
      "            monitoring_output_path)\n",
      "\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mupdate_model\u001B[39;49;00m(\n",
      "        bucket_inference,\n",
      "        model_name,\n",
      "        model_package_group_name,\n",
      "        env,\n",
      "        inference_instance_count,\n",
      "        inference_instance_type,\n",
      "        kms_key,\n",
      "        monitoring_output_path):\n",
      "    \u001B[34mtry\u001B[39;49;00m:\n",
      "        config_name = \u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m-\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m-\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(model_package_group_name, env, datetime.today().strftime(\u001B[33m'\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mY-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mm-\u001B[39;49;00m\u001B[33m%d\u001B[39;49;00m\u001B[33m-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mH-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mM-\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33mS\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m))\n",
      "\n",
      "        LOGGER.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mCreating endpoint configuration \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(config_name))\n",
      "\n",
      "        response_endpoint_config = sagemaker_client.create_endpoint_config(\n",
      "            EndpointConfigName=config_name,\n",
      "            ProductionVariants=[\n",
      "                {\n",
      "                    \u001B[33m\"\u001B[39;49;00m\u001B[33mVariantName\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33m\"\u001B[39;49;00m\u001B[33mAllTraffic\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\n",
      "                    \u001B[33m\"\u001B[39;49;00m\u001B[33mModelName\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: model_name,\n",
      "                    \u001B[33m\"\u001B[39;49;00m\u001B[33mInitialInstanceCount\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: inference_instance_count,\n",
      "                    \u001B[33m\"\u001B[39;49;00m\u001B[33mInstanceType\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: inference_instance_type,\n",
      "                    \u001B[33m\"\u001B[39;49;00m\u001B[33mInitialVariantWeight\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[34m1.0\u001B[39;49;00m\n",
      "                }\n",
      "            ],\n",
      "            DataCaptureConfig={\n",
      "                \u001B[33m'\u001B[39;49;00m\u001B[33mEnableCapture\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: \u001B[34mTrue\u001B[39;49;00m,\n",
      "                \u001B[33m'\u001B[39;49;00m\u001B[33mInitialSamplingPercentage\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: \u001B[34m100\u001B[39;49;00m,\n",
      "                \u001B[33m'\u001B[39;49;00m\u001B[33mDestinationS3Uri\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: \u001B[33m\"\u001B[39;49;00m\u001B[33ms3://\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(bucket_inference, monitoring_output_path),\n",
      "                \u001B[33m'\u001B[39;49;00m\u001B[33mKmsKeyId\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: kms_key,\n",
      "                \u001B[33m'\u001B[39;49;00m\u001B[33mCaptureOptions\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [\n",
      "                    {\n",
      "                        \u001B[33m'\u001B[39;49;00m\u001B[33mCaptureMode\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: \u001B[33m'\u001B[39;49;00m\u001B[33mInput\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\n",
      "                    },\n",
      "                    {\n",
      "                        \u001B[33m'\u001B[39;49;00m\u001B[33mCaptureMode\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: \u001B[33m'\u001B[39;49;00m\u001B[33mOutput\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\n",
      "                    }\n",
      "                ],\n",
      "                \u001B[33m'\u001B[39;49;00m\u001B[33mCaptureContentTypeHeader\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: {\n",
      "                    \u001B[33m'\u001B[39;49;00m\u001B[33mJsonContentTypes\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [\n",
      "                        \u001B[33m'\u001B[39;49;00m\u001B[33mapplication/jsonlines\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m,\n",
      "                    ]\n",
      "                }\n",
      "            }\n",
      "        )\n",
      "\n",
      "        LOGGER.info(response_endpoint_config)\n",
      "\n",
      "        response = sagemaker_client.update_endpoint(\n",
      "            EndpointName=model_package_group_name + \u001B[33m\"\u001B[39;49;00m\u001B[33m-\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m + env,\n",
      "            EndpointConfigName=config_name\n",
      "        )\n",
      "\n",
      "        LOGGER.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mUpdate endpoint \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m-\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(model_package_group_name, env))\n",
      "        LOGGER.info(response)\n",
      "\n",
      "    \u001B[34mexcept\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m e:\n",
      "        stacktrace = traceback.format_exc()\n",
      "        LOGGER.info(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(stacktrace))\n",
      "\n",
      "        \u001B[34mraise\u001B[39;49;00m e\n",
      "\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mget_pipeline\u001B[39;49;00m(\n",
      "    region,\n",
      "    env,\n",
      "    kms_account_id,\n",
      "    kms_alias,\n",
      "    bucket_artifacts,\n",
      "    bucket_inference,\n",
      "    inference_artifact_path,\n",
      "    inference_artifact_name,\n",
      "    inference_instance_count,\n",
      "    inference_instance_type,\n",
      "    model_package_group,\n",
      "    monitoring_output_path,\n",
      "    training_framework_version,\n",
      "    role=\u001B[34mNone\u001B[39;49;00m,\n",
      "    pipeline_name=\u001B[33m\"\u001B[39;49;00m\u001B[33mDeployPipeline\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\n",
      "    \u001B[33m\"\"\"Gets a SageMaker ML Pipeline instance working with on abalone data.\u001B[39;49;00m\n",
      "\u001B[33m\u001B[39;49;00m\n",
      "\u001B[33m    Args:\u001B[39;49;00m\n",
      "\u001B[33m        region: AWS region to create and run the pipeline.\u001B[39;49;00m\n",
      "\u001B[33m        role: IAM role to create and run steps and pipeline.\u001B[39;49;00m\n",
      "\u001B[33m        default_bucket: the bucket to use for storing the artifacts\u001B[39;49;00m\n",
      "\u001B[33m\u001B[39;49;00m\n",
      "\u001B[33m    Returns:\u001B[39;49;00m\n",
      "\u001B[33m        an instance of a pipeline\u001B[39;49;00m\n",
      "\u001B[33m    \"\"\"\u001B[39;49;00m\n",
      "    sagemaker_session = get_session(region, bucket_inference)\n",
      "\n",
      "    \u001B[34mif\u001B[39;49;00m role \u001B[35mis\u001B[39;49;00m \u001B[34mNone\u001B[39;49;00m:\n",
      "        role = get_execution_role()\n",
      "\n",
      "    \u001B[33m\"\"\"\u001B[39;49;00m\n",
      "\u001B[33m        Global parameters\u001B[39;49;00m\n",
      "\u001B[33m    \"\"\"\u001B[39;49;00m\n",
      "\n",
      "    kms_key = get_kms_key(region, kms_account_id, kms_alias)\n",
      "\n",
      "    inference_source_dir = \u001B[33m\"\u001B[39;49;00m\u001B[33ms3://\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(\n",
      "        bucket_artifacts,\n",
      "        inference_artifact_path,\n",
      "        inference_artifact_name\n",
      "    )\n",
      "\n",
      "    \u001B[33m\"\"\"\u001B[39;49;00m\n",
      "\u001B[33m       Get last approved model package\u001B[39;49;00m\n",
      "\u001B[33m   \"\"\"\u001B[39;49;00m\n",
      "\n",
      "    \u001B[34mtry\u001B[39;49;00m:\n",
      "        model_package_approved = get_approved_package(model_package_group)\n",
      "        model_package = describe_model_package(model_package_approved[\u001B[33m\"\u001B[39;49;00m\u001B[33mModelPackageArn\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m])\n",
      "    \u001B[34mexcept\u001B[39;49;00m \u001B[36mException\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m e:\n",
      "        stacktrace = traceback.format_exc()\n",
      "        LOGGER.error(\u001B[33m\"\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(stacktrace))\n",
      "\n",
      "        \u001B[34mraise\u001B[39;49;00m e\n",
      "\n",
      "    \u001B[33m\"\"\"\u001B[39;49;00m\n",
      "\u001B[33m        Get Model from registry\u001B[39;49;00m\n",
      "\u001B[33m    \"\"\"\u001B[39;49;00m\n",
      "\n",
      "    LOGGER.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mCreate SageMaker Model Object\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\n",
      "\n",
      "    model = TensorFlowModel(\n",
      "        entry_point=\u001B[33m\"\u001B[39;49;00m\u001B[33minference.py\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\n",
      "        framework_version=\u001B[36mstr\u001B[39;49;00m(training_framework_version),\n",
      "        source_dir=inference_source_dir,\n",
      "        model_data=model_package[\u001B[33m\"\u001B[39;49;00m\u001B[33mInferenceSpecification\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m][\u001B[33m\"\u001B[39;49;00m\u001B[33mContainers\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m][\u001B[34m0\u001B[39;49;00m][\u001B[33m\"\u001B[39;49;00m\u001B[33mModelDataUrl\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m],\n",
      "        model_kms_key=kms_key,\n",
      "        role=role,\n",
      "        sagemaker_session=sagemaker_session\n",
      "    )\n",
      "\n",
      "    deploy_model(\n",
      "        bucket_inference,\n",
      "        model,\n",
      "        model_package_group,\n",
      "        env,\n",
      "        inference_instance_count,\n",
      "        inference_instance_type,\n",
      "        kms_key,\n",
      "        monitoring_output_path)\n",
      "\n",
      "    \u001B[37m# pipeline instance\u001B[39;49;00m\n",
      "    pipeline = \u001B[34mNone\u001B[39;49;00m\n",
      "\n",
      "    \u001B[34mreturn\u001B[39;49;00m pipeline\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./../mlpipelines/deployment/pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "role_name = \"mlops-sagemaker-execution-role\"\n",
    "role = \"arn:aws:iam::{}:role/{}\".format(boto3.client('sts').get_caller_identity().get('Account'), role_name)\n",
    "\n",
    "env = \"dev\"\n",
    "\n",
    "kms_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "kms_alias = \"ml-kms\"\n",
    "\n",
    "bucket_artifacts = \"\"\n",
    "bucket_inference = \"\"\n",
    "\n",
    "inference_artifact_path = \"artifact/inference\"\n",
    "inference_artifact_name = \"sourcedir.tar.gz\"\n",
    "inference_instance_count = 1\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "model_package_group = \"ml-end-to-end-group\"\n",
    "\n",
    "monitoring_output_path = \"data/monitoring/captured\"\n",
    "\n",
    "training_framework_version = 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compress source code for installing additional python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference.py\n",
      "requirements.txt\n",
      "services/\n",
      "services/.ipynb_checkpoints/\n",
      "services/.ipynb_checkpoints/TranslateService-checkpoint.py\n",
      "services/TranslateService.py\n",
      "services/__init__.py\n",
      "utils/\n",
      "utils/utils.py\n",
      "utils/constants.py\n",
      "utils/__init__.py\n",
      "mkdir: cannot create directory ‘../../dist’: File exists\n",
      "Uploading s3://isengard-bpistone-ml-end-to-end-dev/artifact/inference/sourcedir.tar.gz\n",
      "upload: ../../dist/inference/sourcedir.tar.gz to s3://isengard-bpistone-ml-end-to-end-dev/artifact/inference/sourcedir.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! ./../algorithms/buildspec.sh inference $bucket_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Deployment pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Get pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deployment.pipeline:Identified the latest approved model package: {'ModelPackageGroupName': 'ml-end-to-end-group', 'ModelPackageVersion': 12, 'ModelPackageArn': 'arn:aws:sagemaker:eu-west-1:691148928602:model-package/ml-end-to-end-group/12', 'CreationTime': datetime.datetime(2022, 6, 9, 21, 23, 32, 121000, tzinfo=tzlocal()), 'ModelPackageStatus': 'Completed', 'ModelApprovalStatus': 'Approved'}\n",
      "INFO:deployment.pipeline:{'ModelPackageGroupName': 'ml-end-to-end-group', 'ModelPackageVersion': 12, 'ModelPackageArn': 'arn:aws:sagemaker:eu-west-1:691148928602:model-package/ml-end-to-end-group/12', 'CreationTime': datetime.datetime(2022, 6, 9, 21, 23, 32, 121000, tzinfo=tzlocal()), 'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.eu-west-1.amazonaws.com/tensorflow-inference:2.4-cpu', 'ImageDigest': 'sha256:9655faca3ac7c26a9583ad05a324ca7a58da4af1c14f3d2daae0d217688356a9', 'ModelDataUrl': 's3://isengard-bpistone-ml-end-to-end-dev/models/tensorflow-training-2022-06-09-20-47-38-633/output/model.tar.gz', 'Environment': {'SAGEMAKER_TFS_NGINX_LOGLEVEL': 'info'}}], 'SupportedTransformInstanceTypes': ['ml.m5.xlarge'], 'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.xlarge'], 'SupportedContentTypes': ['application/json'], 'SupportedResponseMIMETypes': ['application/json']}, 'ModelPackageStatus': 'Completed', 'ModelPackageStatusDetails': {'ValidationStatuses': [], 'ImageScanStatuses': []}, 'CertifyForMarketplace': False, 'ModelApprovalStatus': 'Approved', 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:691148928602:user-profile/d-ueaksmq3uudq/bpistone', 'UserProfileName': 'bpistone', 'DomainId': 'd-ueaksmq3uudq'}, 'LastModifiedTime': datetime.datetime(2022, 6, 9, 21, 23, 46, 26000, tzinfo=tzlocal()), 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:691148928602:user-profile/d-ueaksmq3uudq/bpistone', 'UserProfileName': 'bpistone', 'DomainId': 'd-ueaksmq3uudq'}, 'ResponseMetadata': {'RequestId': 'ff9e9179-1279-4616-a4f8-015d3521f638', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ff9e9179-1279-4616-a4f8-015d3521f638', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1356', 'date': 'Fri, 17 Jun 2022 18:48:49 GMT'}, 'RetryAttempts': 0}}\n",
      "INFO:deployment.pipeline:Create SageMaker Model Object\n",
      "INFO:deployment.pipeline:Deploying endpoint ml-end-to-end-group-dev\n",
      "WARNING:sagemaker.deprecations:update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "pipeline = get_pipeline(\n",
    "    region,\n",
    "    env,\n",
    "    kms_account_id,\n",
    "    kms_alias,\n",
    "    bucket_artifacts,\n",
    "    bucket_inference,\n",
    "    inference_artifact_path,\n",
    "    inference_artifact_name,\n",
    "    inference_instance_count,\n",
    "    inference_instance_type,\n",
    "    model_package_group,\n",
    "    monitoring_output_path,\n",
    "    training_framework_version,\n",
    "    role\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}